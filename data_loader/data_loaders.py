from torchvision import datasets, transforms
from base import BaseDataLoader

import random
from collections import Counter
from pathlib import Path

import numpy as np
import torch
from torch.utils.data import Dataset

from base import BaseDataLoader
from data_loader import EcalDataIO
import h5py
import os

# ==== dataloaders ==== #

class MnistDataLoader(BaseDataLoader):
    """
    MNIST data loading demo using BaseDataLoader
    """
    def __init__(self, data_dir, batch_size, shuffle=True, validation_split=0.0, num_workers=1, training=True):
        trsfm = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        self.data_dir = data_dir
        self.dataset = datasets.MNIST(self.data_dir, train=training, download=True, transform=trsfm)
        super().__init__(self.dataset, batch_size, shuffle, validation_split, num_workers)

class data_loader(BaseDataLoader):
    """
    Generates a DL from the existing files - train.pt, test.pt generated by the functions in myutils.py
    """

    def __init__(self, data_dir, batch_size, shuffle=True, validation_split=0.0, num_workers=1, training=True):

        if training == True:
            self.dataset = torch.load(os.path.join('./', 'data', 'train', 'train.pt'))
        else:
            self.dataset = torch.load(os.path.join('./', 'data', 'test', 'test.pt'))

        print("Dataset len: ", len(self.dataset))
        super().__init__(self.dataset, batch_size, shuffle, validation_split, num_workers)


# ================== #

# ==== datasets ==== #

class Bin_energy_data(Dataset):
    """
    General dataset object -
    __getitem__: returns the d_tens (calorimeter matrix data), and the relevant info for the network to learn.
                 Either the num_showers for N training, or the mean energy for E training or final_list for 20bin train.
                 Also returns the num_showers and idx parameter for test purposes.
    """

    def __init__(self, en_dep_file, en_file, moment=1, min_shower_num=0, max_shower_num=10000, file=0):

        self.en_dep = EcalDataIO.ecalmatio(en_dep_file)  # Dict with 100000 samples {(Z,X,Y):energy_stamp}
        self.energies = EcalDataIO.energymatio(en_file)
        del_list = []

        for cntr1, (key1, value1) in enumerate(self.en_dep.items()):
            for cntr2, (key2, value2) in enumerate(value1.items()):
                if key2[1] < 4 or key2[1] > 6 or key2[2] > 10:
                    pass
                    del_list.append((key1, key2))

        
        # self.energies = EcalDataIO.xymatio(en_file)

        for key1, key2 in del_list:
            # self.energies[key1] = list(self.energies[key1])
           
            # del self.en_dep[key1][key2]
            pass

            # del self.energies[key1][cntr2]
            # self.energies[key1] = tuple(self.energies[key1])

        self.moment = moment
        self.file = file

        # Eliminate multiple numbers of some kind
        min_shower_num = 0
        max_shower_num = 20
        if min_shower_num > 0:
            del_list = []
            for key in self.energies:
                if False:
                    del_list.append(key)
            for d in del_list:
                del self.energies[d]
                del self.en_dep[d]
        print()

    def __len__(self):
        return len(self.en_dep)

    
    def __getitem__(self, idx):

        if torch.is_tensor(idx):
            idx = idx.tolist()
        key = list(self.en_dep.keys())[idx]

        d_tens = torch.zeros((110, 11, 21))  # Formatted as [x_idx, y_idx, z_idx]
        tmp = self.en_dep[key]

        for z, x, y in tmp:
            d_tens[x, y, z] = tmp[(z, x, y)]
        d_tens = d_tens.unsqueeze(0)  # Only in conv3d

        en_list = torch.Tensor(self.energies[key])
        num_showers = len(en_list)

        ######### Energy bins Generation ######## - less than
        # hf = h5py.File(os.path.join('./', 'num_classes.h5'), 'r')
        # num_classes = hf.get('dataset_1')
        # num_classes = int(np.array(num_classes))
        # hf.close()

        # x_lim = 13

        # bin_num = num_classes
        # final_list = [0] * bin_num  # The 20 here is the bin number - it may be changed of course.
        # en_list = np.array(en_list.sort().values)[::-1]

        # if len(en_list) >= num_classes:
        #     final_list[:num_classes] = en_list[:num_classes]
        # elif len(en_list) < num_classes:
        #     final_list[:len(en_list)] = en_list
        # final_list = torch.Tensor(final_list)  # Wrap it in a tensor - important for training and testing.
        #########################################

        ######### Energy bins Generation ######## - gaussian
        hf = h5py.File(os.path.join('./', 'num_classes.h5'), 'r')
        num_classes = hf.get('dataset_1')
        num_classes = int(np.array(num_classes))
        hf.close()
        bin_num = num_classes
        final_list = [0] * bin_num  # The 20 here is the bin number - it may be changed of course.
        bin_list = np.linspace(0, 10, bin_num)  # Generate the bin limits
        binplace = np.digitize(en_list, bin_list)  # Divide the list into bins
        bin_partition = Counter(binplace)  # Count the number of showers for each bin.
        for k in bin_partition.keys():
            final_list[int(k) - 1] = bin_partition[k]
        n = sum(final_list)
        # final_list = [f / n for f in final_list]    # Bin Normalization by sum
        final_list = torch.Tensor(final_list)  # Wrap it in a tensor - important for training and testing.
        #########################################

        return d_tens, final_list, num_showers, idx

